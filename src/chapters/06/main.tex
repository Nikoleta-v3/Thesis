\chapter{Best Response Sequences in the Iterated Prisoner's Dilemma}\label{chapter:best_response_sequence}

\begin{center}
    The research reported in this Chapter has been carried out with:

    Axerod-Python library version: 4.2.0 \\
    Associated data set: \cite{Glynatsi2020_sequences} \\ \vspace{.5cm}
\end{center}

\section{Introduction}

In this Chapter best response strategies are explored in the form
of static sequences of moves, in order to generate a large data set of best
response sequences to a collection of opponents.

These best responses are not obtained explicitly but instead are
estimated heuristically using an evolutionary algorithm devised for this purpose. More specifically,
the data set of best response sequences is generated by considering:

\begin{itemize}
    \item IPD matches, and subsequently sequences, of 205 turns.
    \item Opponents from a collection of \numberofstrategiesbestsequences strategies
    available in the APL.
    \item A genetic algorithm to generate a population of candidate best response
    sequences.
\end{itemize}

The data set has been archived and made publicly available~\cite{Glynatsi2020_sequences} and
can be used for further analysis and insights.

The purpose of a large collection of best response sequences, for this thesis, is
to serve as training data in Chapter~\ref{chapter:lstm}.
Chapter~\ref{chapter:lstm} aims to train a recurrent neural network as an IPD
strategy, and there the usage of the bespoke data set will be discussed in more
details. This Chapter is structured as follow:

\begin{itemize}
    \item section~\ref{section:ipd_as_sequences} formalises the use of sequences to express a player in an IPD match
    against a give strategy.
    \item section~\ref{section:genetic_algorithm} describes the genetic algorithm
    used to identify best response
    sequences.
    \item section~\ref{section:generating_sequences} details the process of
    generating best response sequences to a collection of
    \numberofstrategiesbestsequences strategies.
\end{itemize}

\section{Iterated Prisoner Dilemma Strategies as Sequences}\label{section:ipd_as_sequences}

In a finite \(N\) round IPD match, it is possible to define a player that does not react to their opponent by a sequence:

\begin{equation}
    S \in \{C, D\} ^ n
\end{equation}

where \(n\) is the length of the sequence and \(1 \leq n \leq N\).

Strategies that use sequences to play an IPD are already established in the
literature~\cite{Beaufils1997, Knight2016, Li2011, Mittal2009}. More
specifically, the aforementioned works have considered strategies that
periodically play a sequence \(S\). These strategies have been referred to as a
Periodic Player \(S\)~\cite{Li2011, Mittal2009} and as a Cycler
\(S\)~\cite{Knight2016} where \(S \in \{CD, DC, CCD, DDC\}\).
Similarly, this Chapter will consider sequences 
strictly for \(n = N\).

Consider a match of \(N = 10\) between the strategy \(S = \{D, D, D, C, C, C, D,
D, C, C\}\) and a Cooperator. The match between the two strategies is captured
by Table~\ref{table:s_vs_cooperator},

\begin{table}[htb]
\centering
\begin{tabular}{cccccccccccc}
    & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4}  & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8}  & \textbf{9} & \textbf{10} & 
    \(U_{(S, \text{Cooperator})}\) \\ \midrule
    \(S\) & \(D\) & \(D\) & \(D\) & \(C\) & \(C\) & \(C\) & \(D\) & \(D\) & \(C\) & \(C\) & 4.0 \\
    Cooperator & \(C\) & \(C\) & \(C\) & \(C\) & \(C\) & \(C\) & \(C\) & \(C\) & \(C\) & \(C\) & 1.5 \\ \bottomrule
\end{tabular}
\caption{Match between \(S = \{D, D, D, C, C, C, D, D, C, C\}\) and Cooperator.}\label{table:s_vs_cooperator}
\end{table}

where \(U(S, q)\) is the average score per turn the strategies scored against a
given opponent \(q\).

A sequence strategy \(S\) can play against strategies that react to the history,
as demonstrated by Table~\ref{table:s_vs_tft}

\begin{table}[htb]
\centering
\begin{tabular}{cccccccccccc}
    & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4}  & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8}  & \textbf{9} & \textbf{10} &
    \(U_{(S, \text{Tit For Tat})}\) \\ \midrule
    \(S\) & \(D\) & \(D\) & \(D\) & \(C\) & \(C\) & \(C\) & \(D\) & \(D\) & \(C\) & \(C\) & 2.2 \\
    Tit For Tat & \(C\) & \(D\) & \(D\) & \(D\) & \(C\) & \(C\) & \(C\) & \(D\) & \(D\) & \(C\) & 2.2 \\ \bottomrule
\end{tabular}
\caption{Match between \(S = \{D, D, D, C, C, C, D, D, C, C\}\) and Tit For Tat.}\label{table:s_vs_tft}
\end{table}

and against sophisticated strategies, such as Random with a probability \(0.5\)
of cooperating at each turn, Table~\ref{table:s_vs_random}.

\begin{table}[htb]
\centering
\begin{tabular}{cccccccccccc}
    & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4}  & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8}  & \textbf{9} & \textbf{10} &
    \(U_{(S, \text{Random})}\) \\ \midrule
    \(S\) & \(D\) & \(D\) & \(D\) & \(C\) & \(C\) & \(C\) & \(D\) & \(D\) & \(C\) & \(C\) & 2.4 \\
    Random & \(C\) & \(D\) & \(D\) & \(C\) & \(C\) & \(C\) & \(D\) & \(D\) & \(C\) & \(C\) & 1.9 \\ \bottomrule
\end{tabular}
\caption{Match between \(S = \{D, D, D, C, C, C, D, D, C, C\}\) and Random.}\label{table:s_vs_random}
\end{table}

Note that the game of Table~\ref{table:s_vs_random} only captures a single
behaviour of Random against \(S = \{D, D, D, C, C, C, D, D, C, C\}\). Random is
a stochastic strategy, and thus the strategy's series of actions can differ even
when the history of plays is the same. In
section~\ref{section:generating_sequences} computer seeding is introduced.
Computer seeding allows this work to capture and reproduce several
instances/behaviour of stochastic strategies.

A best response sequence to a given opponent \(q\) is the sequence for which the
average score per turn is maximised. More specifically,

\begin{equation}\label{eq:best_response}
    \begin{aligned}
    \max_{S^*}: & U_{(S^*, q)}
    \end{aligned}
\end{equation}

where \(q\) is not a memory-one opponent, but can be any IPD strategy.

Identifying best responses to some opponents can be a trivial problem, for
example the best response sequence for \(N\) turns against Cooperator is
\(\{D\}^N\), however, for most strategies identifying best responses is a
complex problem.

For this reason, best response sequences are not manually identified but instead
a genetic algorithm is used. A background on genetic
algorithms and the exact algorithm that is used in this Chapter are detailed in
section~\ref{section:genetic_algorithm}.

\section{Genetic Algorithm}\label{section:genetic_algorithm}

A genetic algorithm (GA) is a heuristic inspired by the process of natural
selection that belongs to the larger class of evolutionary algorithms. As stated
in~\cite{Whitley1994} GAs encode a potential solution to a
specific problem on a simple chromosome-like data structure, and apply
recombination operators to these structures in such a way as to preserve
critical information. GAs are often viewed as function
optimisers, although the range of problems to which they have been
applied~\cite{Hou1994, Jones1997, Yang1998} is quite broad.

An implementation of a GA begins with a population \(P\) of
potential solutions, a number of generations \(G \in \N\) and a cut-off or
bottleneck \(b < |P|\). At each generation the algorithm scores and potentially
removes each member of the population \(p_i \in P\). This is done by using a
mapping from a member of the population to an ordered set based on an evaluation
function \(f\), such that \(f(p_i) \to \R\). At the conclusion of each
generation the top \(b\) ranking members (or proportion of members) by score are
kept and the rest are discarded. By doing so, critical information regarding
the successful candidates is preserved and the population rebuilds on it using a
series of \textit{crossovers} and \textit{mutations}.

\begin{itemize}
    \item Crossovers take in 2 members of the population and return a new member
    based on some ``genes'' of the 2 selected members (commonly refereed to as parents).
    \item Mutations allow a change in a single member of the population. Mutation
    is commonly associated with a probability \(p_m\) which is the probability
    of a mutation happening, either at the individual or at each gene of the
    individual.
\end{itemize}

GA has the ability to avoid being trapped in local optimal solution like
traditional methods. This ability is due to the properties of crossover and
mutation. Crossover adds variance in the population by taking properties of both
parent members and introducing the evaluation of their combination to the
solution space. The algorithm can search multiple local optimal at once with its
population and start to identify the global optimal as members become more
optimised. Moreover, if an a member is trapped in a local optimal with its
fitness score, a mutation of a certain feature could potentially move their
position in the solution space to a better local optimal.

A diagrammatical representation of a generic GA is given in Figure~\ref{fig:ga_flow_diagram}.

\begin{figure}[!htbp]
    \centering
    \includestandalone[width=\textwidth]{src/chapters/06/tex/ga_flow_diagram}
    \caption{Generic flow diagram of a GA.}\label{fig:ga_flow_diagram}
\end{figure}

The purpose of a GA in this Chapter is to estimate the best response sequence to
a given opponent. Consequently, the members of the population correspond to
sequences \(S\) of length \(N\), and the evaluation function corresponds to the
average score per turn of the sequence against its opponent. The detailed GA of
this work is given by Algorithm~\ref{algorithm:genetic_algorithm}.

\begin{algorithm}[!htbp]
    \SetAlgoLined
    \KwIn{\(q, N, b, p_m, G, s\)}
    \KwOut{Population at the last generation and the members' scores}
     \Begin{
     create initial population (Algorithm~\ref{algorithm:initial_population}) of members \(S\), where \(|S| = N\) and \(|P| = s\)\\
     \While{\(g_{i} < G\)}{
        score each member based on \(U_{(S, q)}\) for \(S \in P\) \\
        sort population based on scores \\
        keep \(b\) top members \\
        \While{\(|P|\) \(< s\)}{
            select 2 random members \\
            use members to create new member through crossover\\
            \For{gene in new member}{
            mutate gene with probability \(p_m\)}
            add new member to population\\
            }
     }}
     \caption{Get population of potential best response sequences}\label{algorithm:genetic_algorithm}
\end{algorithm}

The initial population is generated using Algorithm~\ref{algorithm:initial_population}.
Using a starting population of random guesses is a generally common approach in
the GA literature~\cite{Hou1994}. However, there is efficiency in using non
random starting populations~\cite{Drezner2005, Osaba2014}.
In the IPD the best responses to a board number of well established strategies
are either of the deterministic plays of always cooperating or defecting.
Thus, both of these are included in the initial population and the rest of
the population is filled with a combination of these two strategies.

\begin{algorithm}[!htbp]
        \SetAlgoLined
        \KwIn{\(s, N\)}
        \KwOut{A population of size \(s\).}
    
        \Begin{
        set of cuts $\gets$ \(s\) evenly spaced numbers over \([1, N]\) \\
        \For{\(c \in\) set of cuts}{
            first new member $\gets$  \(\{C\}^{c} \cup \{D\}^{N-c}\) \\
            second new member $\gets$  \(\{D\}^{c} \cup \{C\}^{N-c}\) \\
            add both members to population
        }
            }
     \caption{Create initial population of individuals \(S\)}\label{algorithm:initial_population}
\end{algorithm}

The crossover between two members occurs by randomly selecting a crossover point
less than \(N\), and the new member inheriting the genes right to the crossover point from
the first parent and the left genes to the crossover point from the second
parent. A mutation is applied to new member before they are added to the
population. During mutation there is probability \(p_m\) that each gene of the
new member is flipped.

The GA of this Chapter has been implemented in the programming language Python
and it has been organised into a open source package called
\mintinline{python}{sequence_sensei} available at. %TODO archive
Figure~\ref{fig:get_initial_population} demonstrates the implementation of
Algorithm~\ref{algorithm:initial_population} in the package. Moreover,
Figure~\ref{fig:get_initial_population_example} gives a usage example of
generating a starting population of a given size using
\mintinline{python}{sequence_sensei}.
The bottom members of the populations are the sequences \(\{1\}^N\) and
\(\{0\}^N\), where \(0 \to D\) and \(1 \to C\), as expected. In
Figure~\ref{fig:get_initial_population_example} it is also illustrated how APL
can map binary numbers to the IPD actions.

\begin{figure}[!htbp]
\begin{sourcepy}
import numpy as np
def get_initial_population(half_size_of_population, sequence_length):
    """
    Generates an initial population of sequences. Note that the length
    of the population which is being generated is 2 * half_size_of_population.
    """
    cuts = np.linspace(1, sequence_length, half_size_of_population, dtype=int)
    sequences = []
    for cut in cuts:
        sequences.append(
            [1 for _ in range(cut)] + [0 for _ in range(sequence_length - cut)]
        )
        sequences.append(
            [0 for _ in range(cut)] + [1 for _ in range(sequence_length - cut)]
        )

    return sequences
\end{sourcepy}
\caption{Source code of the \mintinline{python}{get_initial_population} function
implemented in \mintinline{python}{sequence_sensei}.}\label{fig:get_initial_population}
\end{figure}

\begin{figure}[!htbp]
    \begin{usagepy}
>>> import sequence_sensei as ss
>>> import numpy as np

>>> initial_population = ss.get_initial_population(half_size_of_population=5, sequence_length=8)
>>> np.matrix(initial_population)
matrix([[1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0]])

>>> import axelrod as axl
>>> np.matrix([[axl.Action(gene) for gene in member] for member in initial_population])
matrix([[C, D, D, D, D, D, D, D],
        [D, C, C, C, C, C, C, C],
        [C, C, D, D, D, D, D, D],
        [D, D, C, C, C, C, C, C],
        [C, C, C, C, D, D, D, D],
        [D, D, D, D, C, C, C, C],
        [C, C, C, C, C, C, D, D],
        [D, D, D, D, D, D, C, C],
        [C, C, C, C, C, C, C, C],
        [D, D, D, D, D, D, D, D]], dtype=object)

\end{usagepy}
\caption{Example of using \mintinline{python}{get_initial_population} to
generate a population of \(s=10\) and \(N=8\).}\label{fig:get_initial_population_example}
\end{figure}

The implementations of the crossover and mutation properties are given in Figure~\ref{fig:crossover_mutation},
furthermore, an example of a crossover between a Cooperator and an Alternator and the
mutation of the crossover's result is given in Figure~\ref{fig:crossover_mutation}.

\begin{figure}[!htbp]
\begin{sourcepy}
import random

def crossover(sequence_one, sequence_two):
    sequence_length = len(sequence_one)
    crossover_point = random.randint(0, sequence_length)

    return sequence_one[:crossover_point] + sequence_two[crossover_point:]

def mutation(gene, mutation_probability):
    if random.random() < mutation_probability:
        return abs(gene - 1)
    return gene
\end{sourcepy}
\caption{Source code of crossover and mutation properties in \mintinline{python}{sequence_sensei}.}\label{fig:crossover_mutation}
\end{figure}

\begin{figure}[!htbp]
\begin{usagepy}
>>> import random
>>> import sequence_sensei as ss

>>> turns = 10
>>> cooperator = [1 for _ in range(turns)]
>>> alternator = [i % 2 for i in range(turns)]

>>> random.seed(1)
>>> new_member = ss.crossover(cooperator, alternator)
>>> new_member
[1, 1, 0, 1, 0, 1, 0, 1, 0, 1]

>>> random.seed(1)
>>> [ss.mutation(gene, mutation_probability=0.05) for gene in new_member]
[1, 1, 0, 1, 0, 1, 0, 1, 0, 0]

\end{usagepy}
\caption{Example of crossover between a Cooperator and an Alternator, and an example
of the mutation property.}\label{fig:crossover_mutation}
\end{figure}

The main function implemented in \mintinline{python}{sequence_sensei} for
performing a GA is the \mintinline{python}{evolved} function. The function has
several input arguments which correspond to the input as given in
Algorithm~\ref{algorithm:genetic_algorithm}. Details for the parameters' values
and the scoring process of the populations, and the process of generating a best
response sequence data set are presented in
section~\ref{section:generating_sequences}.

\section{Generating Best Response Sequences}\label{section:generating_sequences}

In order to generate the data set of best response sequences a collection of
\numberofstrategiesbestsequences strategies is considered as opponents. This
collection of strategies is accessible from the APL, and moreover,
the library is used to simulate the IPD games and to calculate the score
of the sequence strategies.

The APL projects contains a strategy called Cycler which plays a given sequence
\(S\). The Cycler strategy is considered to simulate the matches between the
\numberofstrategiesbestsequences strategies and the potential best response
sequences. An example of using the strategy Cycler is given in
Figure~\ref{fig:apl_simulations_cycler}. Figure~\ref{fig:apl_simulations_cycler}
illustrates how the matches of Tables~\ref{table:s_vs_cooperator}
and~\ref{table:s_vs_tft} can be simulated using APL, and moreover, how the score
of each strategy is accessible once the match has been performed.

\begin{figure}[!htbp]
    \begin{usagepy}
>>> import axelrod as axl

>>> players = [axl.Cycler('DDDCCCDDCC'), axl.Cooperator()]
>>> match = axl.Match(players, turns=10)
>>> match.play()
[(D, C), (D, C), (D, C), (C, C), (C, C), (C, C), (D, C), (D, C), (C, C), (C, C)]

>>> match.final_score_per_turn()
(4.0, 1.5)

>>> players = [axl.Cycler('DDDCCCDDCC'), axl.TitForTat()]
>>> match = axl.Match(players, turns=10)
>>> match.play()
[(D, C), (D, D), (D, D), (C, D), (C, C), (C, C), (D, C), (D, D), (C, D), (C, C)]

>>> match.final_score_per_turn()
(2.2, 2.2)

\end{usagepy}
\caption{Simulating matches using a Cycler to play a given sequence.}\label{fig:apl_simulations_cycler}
\end{figure}

The \numberofstrategiesbestsequences strategies used in this Chapter are found
in the Appendix alongside a citation of their paper of origin.
From these \numberofstrategiesbestsequences strategies, 62 are stochastic and
135 are deterministic. As it has been established, the outcome of a match
between two deterministic strategies never changes, as longs as the environment
does not include noise. In comparison, the outcomes of a match which includes even a single
stochastic strategy can be significantly different. In order to
capture distinct behaviours of stochastic strategies, and to be able to
reproduce the results of a match with a stochastic strategy, this work considers
\textit{computer seeding}. In Python a computer seed is used to initialise the
pseudo random number generator. Seeds are set before generating a random number,
and if the same seed is used on initialisation then the random output remains
the same. Thus, as long as a match is being seeded the behaviour of a stochastic
strategy can be reproduced, and different seeds lead to different series of
random numbers.

Consider the example of Random of Table~\ref{table:s_vs_random}. In
Figure~\ref{fig:random_apl_example} a different seed is being set five times
before the match between the sequence strategy and Random. The actions of the
Random strategy are different each time.

\begin{figure}[!htbp]
    \begin{usagepy}
>>> players = [axl.Cycler('DDDCCCDDCC'), axl.Random()]
>>> for seed in range(5):
...   axl.seed(seed)
...   match = axl.Match(players, turns=10)
...   actions = match.play()
...   print(actions, match.final_score_per_turn())
...   print("================================================================================")
[(D, D), (D, D), (D, C), (C, C), (C, D), (C, C), (D, D), (D, C), (C, C), (C, D)] (2.2, 2.2)
================================================================================
[(D, C), (D, D), (D, D), (C, C), (C, C), (C, C), (D, D), (D, D), (C, C), (C, C)] (2.4, 1.9)
================================================================================
[(D, D), (D, D), (D, C), (C, C), (C, D), (C, D), (D, D), (D, C), (C, D), (C, D)] (1.6, 2.6)
================================================================================
[(D, C), (D, D), (D, C), (C, D), (C, D), (C, C), (D, C), (D, D), (C, C), (C, C)] (2.6, 2.1)
================================================================================
[(D, C), (D, C), (D, C), (C, C), (C, C), (C, C), (D, D), (D, D), (C, D), (C, C)] (2.9, 1.9)
================================================================================

\end{usagepy}
\caption{Example code of using seeding to generate different instances of Random.
The above code snipped will always have the same output even if the matches are
``random'' because a computer seed is set before performing the
match.}\label{fig:random_apl_example}
\end{figure}

The data generating process selects one opponent from the collection of the
\numberofstrategiesbestsequences strategies and performs a series of GAs with
different parameters' values. The population at the last generation as well as
the scores of the sequences are exported in a csv file. Each time an opponent is
selected it is evaluated whether that opponent is deterministic or not. For
deterministic opponents the process is repeated only once, however, for
stochastic opponents a series of GAs are performed for 10 different seeded
version of the strategy. Thus, in total the process is repeated (\(135 + 62
\times 10\)) 755 times. Only a single value for the length of the
sequences has been considered and that the value of \(N=205\). A diagrammatical
representation of the collection process is given by
Figure~\ref{fig:data_generating_process_diagram}.

\begin{figure}[!htbp]
    \centering
    \includestandalone[width=\textwidth]{src/chapters/06/tex/data_generating_diagram}
    \caption{Diagrammatical representation of the best response
    sequences generating process.}\label{fig:data_generating_process_diagram}
\end{figure}

A series of GAs are performed for each of the of 755 distinct \(q\)s with
different parameter values. These values alongside a short explanation of the
parameters are given in Table~\ref{table:parameters_summary}. Moreover,
an example of an exported generation (which is exported in a csv format)
is given by Table~\ref{table:ouput_summary}.

\begin{table}[!htbp]
    \begin{center}
    \resizebox{.7\textwidth}{!}{
    \begin{tabular}{lllc} \toprule
    Parameter's symbol& Explanation                   & Values \\ \midrule
    \(N\)             & number of turns               & \(205\) \\
    \(p_m\)           & probability of gene mutating  & \(\{0.01, 0.05, 0.1\}\)\\
    \(b\)             & bottle neck                   & \(10, 20\) \\
    \(G\)             & number of generations         & \(2000\) \\
    \(s\)             & size of a population          & \(20, 30, 40\) \\ \bottomrule
    \end{tabular}}
    \end{center}
    \caption{The parameters of the genetic algorithm.}\label{table:parameters_summary}
\end{table}

\begin{table}[!htbp]
    \resizebox{\textwidth}{!}{
    \input{src/chapters/06/tex/output_table.tex}}
    \caption{An exampled of an exported csv. The output has \(s\) number of rows because
    each row contains information for each member of the population. Alternator is
    a deterministic strategy, consequently,
    the value of seed in NaN.}\label{table:ouput_summary}
\end{table}

An output (Table~\ref{table:ouput_summary}) has \(s\) number of rows because
each row contains information for each member of the population. At each of these
members are a potential best response sequence to the given opponent they have
been trained against. There are cases that more than a single member of the
final population is a best response. This is intuitive and expected following the
work of Chapter~\ref{chapter:memory_one} where it was proven that the utility
of even a memory one player is not concave, and thus, there is not one single
point which maximises the score against an IPD opponent. The total amount
of best response sequences which has been retrieved following the process
discussed in this section is 5503.

\section{Chapter Summary}

This Chapter has explored the concept of best responses in the IPD game in the
form of static sequences of moves. It has identified a successful method of
generating best response sequences using an evolutionary algorithm and executed
analysis to find these solutions to the majority of opponents listed in the APL.

More specifically, a total of \numberofstrategiesbestsequences opponents from
APL were used. Several of the strategies in the project are stochastic and
computer seeded version of these strategies were used to explore their different
behaviours. A given opponent can have more than a single best response to it, and
as a result, a total of 5503 best response sequences to a total of 755 opponents
have been calculated. The data set has been archive and is available at
\cite{Glynatsi2020_sequences}.

The main purpose of this Chapter has been to generate the bespoke data set,
which is to be used as training data set in the following Chapter.
