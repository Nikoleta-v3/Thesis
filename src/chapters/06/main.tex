\chapter{Best Response Sequences in the Iterated Prisoner's Dilemma}\label{chapter:best_response_sequence}

\begin{center}
    The research reported in this Chapter has been carried out with:

    Axerod-Python library version: 4.2.0 \\
    Associated data set: \cite{Glynatsi2020_sequences} \\ \vspace{.5cm}
\end{center}

\section{Introduction}

In this Chapter best response strategies are explored in the form
of static sequences of moves, in order to generate a large data set of best
response sequences to a collection of opponents.

The data set is generated by considering best response sequences in finite IPD
matches of 205 turns against \numberofstrategiesbestsequences strategies
available in the APL. These best response sequences are not obtained explicitly
but instead are estimated heuristically using a genetic algorithm devised for
this purpose.

The purpose of a large collection of best response sequences is
to serve as training data in Chapter~\ref{chapter:lstm} which aims to train a
recurrent neural network as an IPD strategy. In Chapter~\ref{chapter:lstm} the
usage of the bespoke data set, which has been archived and made publicly
available~\cite{Glynatsi2020_sequences}, is be discussed in more details. This
Chapter is structured as follow:

\begin{itemize}
    \item section~\ref{section:ipd_as_sequences} formalises the use of sequences to express a player in a
    finite IPD match.
    \item section~\ref{section:genetic_algorithm} describes the genetic algorithm
    used to estimate best response
    sequences.
    \item section~\ref{section:generating_sequences} details the process of
    generating best response sequences to a collection of
    \numberofstrategiesbestsequences strategies, and presents a brief analysis
    of its results.
\end{itemize}

\section{Iterated Prisoner Dilemma Strategies as Sequences}\label{section:ipd_as_sequences}

In a finite \(N\) round IPD match a player that does not react to their opponent
can be defined by a sequence,

\begin{equation}
    S \in \{C, D\} ^ n, \text{where } 1 \leq n \leq N.
\end{equation}

Strategies that base their actions on sequences are already established in the
literature~\cite{Beaufils1997}, such as Periodic Player \(CD\), Periodic Player \(DC\),
Periodic Player \(CCD\) and Periodic Player \(DDC\)~\cite{Li2011, Mittal2009},
or as referred to in~\cite{Knight2016} Cycler \(CD\), Cycler \(DC\),
Cycler \(CCD\) and Cycler \(DDC\). These are strategies that play a given sequences
periodically, however, the strategies concerned with here play a given
sequence only once. Thus \(n = N\).

As an example consider a match of 10 turns between the strategy \(S = \{D, D, D,
C, C, C, D, D, C, C\}\) and Cooperator. The match between the two strategies
is captured by Table~\ref{table:s_vs_cooperator} where \(U(s_1, s_2)\in
\mathbb{R}^2\) is the average score per turn scored by strategies \(s_1\) and
\(s_2\).

\begin{table}[htb]
\centering
\begin{tabular}{cccccccccccc}
    & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4}  & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8}  & \textbf{9} & \textbf{10} & 
    \(U(S, \text{Cooperator})\) \\ \midrule
    \(S\) & \(D\) & \(D\) & \(D\) & \(C\) & \(C\) & \(C\) & \(D\) & \(D\) & \(C\) & \(C\) & 4.0 \\
    Cooperator & \(C\) & \(C\) & \(C\) & \(C\) & \(C\) & \(C\) & \(C\) & \(C\) & \(C\) & \(C\) & 1.5 \\ \bottomrule
\end{tabular}
\caption{The interactions of a 10 turns match between \(S = \{D, D, D, C, C, C, D, D, C, C\}\) and
Cooperator as well as the average score per turn achieved by each strategy.}\label{table:s_vs_cooperator}
\end{table}

A sequence strategy \(S\) can play against strategies that react to the history,
for example against Tit Tor Tat as demonstrated by Table~\ref{table:s_vs_tft},

\begin{table}[htb]
\centering
\begin{tabular}{cccccccccccc}
    & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4}  & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8}  & \textbf{9} & \textbf{10} &
    \(U(S, \text{Tit For Tat})\) \\ \midrule
    \(S\) & \(D\) & \(D\) & \(D\) & \(C\) & \(C\) & \(C\) & \(D\) & \(D\) & \(C\) & \(C\) & 2.2 \\
    Tit For Tat & \(C\) & \(D\) & \(D\) & \(D\) & \(C\) & \(C\) & \(C\) & \(D\) & \(D\) & \(C\) & 2.2 \\ \bottomrule
\end{tabular}
\caption{The interactions and average score per turn of a 10 turns match between
\(S = \{D, D, D, C, C, C, D, D, C, C\}\) and Tit For Tat.}\label{table:s_vs_tft}
\end{table}

and against stochastic strategies such as Random. Random cooperates with a
probability of \(0.5\) at each turn, and thus the actions of the strategy are
probabilistic and not deterministic. Due to that Random's actions can differ
between repetitions. Random is a strategy that does react to the history, however,
the actions of stochastic strategies that react to the
history also differ between repetitions even when the history of the match is
the same. Tables~\ref{table:s_vs_random} and~\ref{table:s_vs_random_2} both
capture a match between \(S\) and a Random player. For a match of 10 turns
Random has a total of \(2^{10}\) possible plays. %TODO check my maths is right possible plays.
In order to capture several different plays of stochastic strategies computer
seeding is used in section~\ref{section:generating_sequences}.

\begin{table}[htb]
\centering
\begin{tabular}{cccccccccccc}
    & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4}  & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8}  & \textbf{9} & \textbf{10} &
    \(U(S, \text{Random})\) \\ \midrule
    \(S\) & \(D\) & \(D\) & \(D\) & \(C\) & \(C\) & \(C\) & \(D\) & \(D\) & \(C\) & \(C\) & 2.2 \\
    Random & \(D\) & \(D\) & \(C\) & \(C\) & \(D\) & \(C\) & \(D\) & \(C\) & \(C\) & \(D\) & 2.2 \\ \bottomrule
\end{tabular}
\caption{The interactions and average score per turn of a 10 turns match between
\(S = \{D, D, D, C, C, C, D, D, C, C\}\) and Random.}\label{table:s_vs_random}
\end{table}

\begin{table}[htb]
\centering
\begin{tabular}{cccccccccccc}
    & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4}  & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8}  & \textbf{9} & \textbf{10} & \(U(S, \text{Random})\) \\ 
    \midrule 
    \(S\) & \(D\) & \(D\) & \(D\) & \(C\) & \(C\) & \(C\) & \(D\) & \(D\) & \(C\) & \(C\) & 2.4 \\
    Random & \(C\) & \(D\) & \(D\) & \(C\) & \(C\) & \(C\) & \(D\) & \(D\) & \(C\) & \(C\) & 1.9 \\ \bottomrule
\end{tabular}
\caption{The interactions and average score per turn of a 10 turns match between
\(S = \{D, D, D, C, C, C, D, D, C, C\}\) and Random. The actions make by Random
are different to that of Table~\ref{table:s_vs_random}.}\label{table:s_vs_random_2}
\end{table}

As discussed in Chapters~\ref{chapter:introduction} and~\ref{chapter:memory_one},
a best response strategy is a strategy that achieves that most favourable outcome.
Thus a best response sequence against a given opponent \(Q\) corresponds
to a sequence \(S^*\) for which the average score per turn is maximised,
as given in~(\ref{eq:best_response_sequence}).

\begin{equation}\label{eq:best_response_sequence}
    \begin{aligned}
    \max_{S^*}: & U(S^*, Q)
    \end{aligned}
\end{equation}

Identifying best responses to some opponents can be a trivial problem. The
optimal sequence against a Cooperator is in an all \(D\) sequence. In fact the
sequence \(\{\underbrace{D, \dots, D}_{N}\}\) is a best response
against any sequence player whose plays are independent of the history.
However,
for some strategies identifying best responses is a complex problem. Note that

Additionally, there are strategies that have multiple best response sequences. For
instance the strategy Adaptive introduced in~\cite{Li2011}. Adaptive opens by
playing a sequence of \(\{\underbrace{C, \dots, C}_{6}\}\) followed by a
sequence of \(\{\underbrace{D, \dots, D}_{5}\}\). The strategy then proceeds to
play either \(C\) or \(D\) depending on which action had a higher total score
for the strategy (the total score is recalculated at each turn). A sequence
maximises its average score against Adaptive by locking the strategy into
unconditional cooperations following its opening 11 turns sequence while the sequence
defects. In order
for cooperation to be the most favourable action for Adaptive the strategy needs to achieve
two mutual cooperations at its opening sequence. That is because the score achieved
by cooperating \(2 \times 3 + 4 \times 0 = 6\) is greater than the
score achieved by defecting \(1 \times 5 = 5\).
Any sequence which incorporates two cooperations in the first 6 turns
and defects thereafter is a best response sequence to Adaptive. Thus, there
can be \(2^{6}\) best response sequences to the strategy, for example \(S_{1}^*\) and \(S_{2}^*\),

\[S_{1}^* = \{C, D, D, D, D, C, D, D, D, D, D, D, D, D, D\}\]

\[S_{2}^* = \{D, D, C, C, D, D, D, D, D, D, D, D, D, D, D\}\]

where \(U(S_{1}^*, \text{Adaptive}) = U(S_{2}^*, \text{Adaptive}) = 3.4\).

Due to identifying best response sequences to some opponents being a complex
problem, and moreover, multiple best response sequences existing for some
opponents the best response sequences are not manually identified. Instead a
genetic algorithm is used to estimate them. A background on genetic algorithms
as well as the details of the specific genetic algorithm devised for this Chapter
are presented in the following section.

\section{Genetic Algorithm}\label{section:genetic_algorithm}

A genetic algorithm (GA) is a heuristic inspired by the process of natural
selection that belongs to the larger class of evolutionary algorithms. As stated
in~\cite{Whitley1994} GAs encode a potential solution to a
specific problem on a simple chromosome-like data structure, and apply
recombination operators to these structures in such a way as to preserve
critical information. GAs are often viewed as function
optimisers, although the range of problems to which they have been
applied is quite broad~\cite{Hou1994, Jones1997, Yang1998}.

An implementation of a GA begins with a \textit{population} \(P\) of
potential solutions, a number of \textit{generations} \(G \in \N\) and a cut-off or
\textit{bottleneck} \(b < |P|\). At each generation the algorithm scores and potentially
removes each member of the population \(p_i \in P\). This is done by using a
mapping from a member of the population to an ordered set based on an evaluation
function \(f\), such that \(f(p_i) \to \R\), and by only keeping the top \(b\)
ranking members (or proportion of members) by score at the end of each
generation. The rest of the member are discarded. By keeping the top ranked
individuals critical information regarding the successful candidates is
preserved and the population rebuilds on it by using a series of
\textit{crossovers} and \textit{mutations}.

\begin{itemize}
    \item During a crossover 2 members of the population are selected, and a new
    member is created based on combination of their ``genes''. The 2 selected members
    are commonly referred to as parents.
    \item Mutation is a probabilistic change that occurs at an individual,
    Mutation is commonly associated with a probability, denoted as \(p_m\).
    \(p_m\) is the probability of a mutation happening, either at the individual
    or at each gene of the individual.
\end{itemize}

A diagrammatical representation of a generic GA is given in Figure~\ref{fig:ga_flow_diagram}.

\begin{figure}[!htbp]
    \centering
    \includestandalone[width=.77\textwidth]{src/chapters/06/tex/ga_flow_diagram}
    \caption{Generic flow diagram of a GA.}\label{fig:ga_flow_diagram}
\end{figure}

The purpose of a GA here is to estimate a best response sequence to
a given opponent \(Q\). Consequently, the members of the population correspond to
sequences,

\[S_i \in P \text{ where } |S_i| = N\]

and the evaluation function corresponds to the
average score per turn of a sequence against \(Q\),

\[U(S_i, Q) \to \R \text{ for } S_i \in P.\]

More specifically, the exact GA used in this Chapter is given
by Algorithm~\ref{algorithm:genetic_algorithm}.

\begin{algorithm}[!htbp]
    \SetAlgoLined
    \KwIn{\(Q, N, b, p_m, G, K\)}
    \KwOut{The populations at each generation and the members' scores}
     \Begin{
     create initial population (Algorithm~\ref{algorithm:initial_population}) of members \(S\), where \(|S| = N\) and \(|P| = K\)\\
     \While{\(g_{i} < G\)}{
        score each member based on \(U(S_i, Q)\) for \(S_i \in P\) \\
        sort population based on scores \\
        keep \(b\) top members \\
        \While{\(|P|\) \(< K\)}{
            select 2 random members \\
            use members to create new member through crossover\\
            \For{gene in new member}{
            mutate gene with probability \(p_m\)}
            add new member to population\\
            }
     }}
     \caption{GA for estimating best response sequences to a given opponent \(Q\).}\label{algorithm:genetic_algorithm}
\end{algorithm}

The initial population is created using Algorithm~\ref{algorithm:initial_population}.

\begin{algorithm}[!htbp]
    \SetAlgoLined
    \KwIn{\(K, N\)}
    \KwOut{A population of size \(K\).}

    \Begin{
    set of cuts $\gets$ \(K\) evenly spaced numbers over \([1, N]\) \\
    \For{\(c \in\) set of cuts}{
        first new member $\gets$  \(\{\underbrace{C, \dots, C}_{c}, \underbrace{D, \dots, D}_{N-c}\}\) \\
        second new member $\gets$ \(\{\underbrace{D, \dots, D}_{c}, \underbrace{C, \dots, C}_{N-c}\}\) \\
        add both members to population
    }
        }
 \caption{Create initial population of individuals \(S\)}\label{algorithm:initial_population}
\end{algorithm}

Using a starting population of random guesses is a generally common approach in
the GA literature~\cite{Hou1994}. However, there is efficiency in using non
random starting populations~\cite{Drezner2005, Osaba2014}.
As discussed in section~\ref{section:ipd_as_sequences} the best response sequence
to any strategy that does not react to the history is a Defector. Moreover,
the best response sequence to several strategies include sequences mainly dominated
by cooperation expect in the last turns. These are
sequences that have been incorporated in the initial population. More specifically,
Algorithm~\ref{algorithm:initial_population} consider all the possible combinations
of:

\[\bullet \{\underbrace{C, \dots, C}_{c}, \underbrace{D, \dots, D}_{N-c}\} \text{ for } c \in \text{evenly spaced numbers over } [1, N] \text{ and}\]
\[\bullet \{\underbrace{D, \dots, D}_{c}, \underbrace{C, \dots, C}_{N-c}\} \text{ for } c \in \text{evenly spaced numbers over } [1, N].\]

The GA of Algorithm~\ref{algorithm:genetic_algorithm} has been implemented in the programming language Python
and it has been organised into a open source package called
\mintinline{python}{sequence_sensei} available at. %TODO archive
The properties of creating an initial population, crossover and mutation have been
implemented as individual functions.
The implementation of Algorithm~\ref{algorithm:initial_population} in the package
is given by Figure~\ref{fig:get_initial_population}.

\begin{figure}[!htbp]
\begin{sourcepy}
import numpy as np
def get_initial_population(half_size_of_population, sequence_length):
    """
    Generates an initial population of sequences. Note that the length
    of the population which is being generated is 2 * half_size_of_population.
    """
    cuts = np.linspace(1, sequence_length, half_size_of_population, dtype=int)
    sequences = []
    for cut in cuts:
        sequences.append(
            [1 for _ in range(cut)] + [0 for _ in range(sequence_length - cut)]
        )
        sequences.append(
            [0 for _ in range(cut)] + [1 for _ in range(sequence_length - cut)]
        )

    return sequences
\end{sourcepy}
\caption{Source code for the function \mintinline{python}{get_initial_population}
implemented in \mintinline{python}{sequence_sensei} which is used to create an
initial population of a given size.}\label{fig:get_initial_population}
\end{figure}

Figure~\ref{fig:get_initial_population_example} gives an example of creating an
initial population using the package. Note that the sequences are of 0s and 1s
and not IPD actions. The APL project can map binary number to actions such that
\(0 \to D\) and \(1 \to C\). This is also demonstrated in
Figure~\ref{fig:get_initial_population_example}.

\begin{figure}[!htbp]
    \begin{usagepy}
>>> import sequence_sensei as ss
>>> import numpy as np

>>> initial_population = ss.get_initial_population(half_size_of_population=5, sequence_length=8)
>>> np.matrix(initial_population)
matrix([[1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0]])

>>> import axelrod as axl
>>> np.matrix([[axl.Action(gene) for gene in member] for member in initial_population])
matrix([[C, D, D, D, D, D, D, D],
        [D, C, C, C, C, C, C, C],
        [C, C, D, D, D, D, D, D],
        [D, D, C, C, C, C, C, C],
        [C, C, C, C, D, D, D, D],
        [D, D, D, D, C, C, C, C],
        [C, C, C, C, C, C, D, D],
        [D, D, D, D, D, D, C, C],
        [C, C, C, C, C, C, C, C],
        [D, D, D, D, D, D, D, D]], dtype=object)

\end{usagepy}
\caption{Example of using \mintinline{python}{get_initial_population} to
generate a population of \(s=10\) and \(N=8\).}\label{fig:get_initial_population_example}
\end{figure}

In the GA, as given by Algorithm~\ref{algorithm:genetic_algorithm}, the
crossover occurs by randomly selecting two member of the population, while \(|P|
< K\), and randomly selecting a crossover point. Note that the crossover
point is smaller than \(N\). The new member initially inherits the
genes right to the crossover point of the first parent, and then the left
genes to the crossover point of the second parent.

For instance, given two member of the population \(S_1 = \{C, C, C, C, C, C, C, C, C, C\}\)
and \(S_2 = \{C, D, C, D, C, D, C, D, C, D\}\) and given that the crossover point is
4:

\begin{itemize}
    \item The right genes to the crossover point for \(S_1\) are \(\{\underbrace{C, C, C, C}_{\text{to inherit}}, C, C, C, C, C, C\}.\)
    \item The left genes to the crossover point for \(S_2\) are \(\{C, D, C, D,\underbrace{C, D, C, D, C, D}_{\text{to inherit}}\}.\)
\end{itemize}

Thus, the new member \(S_3\) is given by,

\[S_3 = \{\underbrace{C, C, C, C}_{\text{from } S_1}, \underbrace{C, D, C, D, C, D}_{\text{from } S_2}\}.\]

The implementation of crossover in \mintinline{python}{sequence_sensei} is given
by Figure~\ref{fig:crossover_implementation}, and Figure~\ref{fig:crossover_usage}
demonstrates the usage of the \mintinline{python}{crossover} function to crossover
\(S_1\) and \(S_2\).

\begin{figure}[!htbp]
\begin{sourcepy}
import random

def crossover(sequence_one, sequence_two):
    sequence_length = len(sequence_one)
    crossover_point = random.randint(0, sequence_length)

    return sequence_one[:crossover_point] + sequence_two[crossover_point:]

\end{sourcepy}
\caption{Source code of the \mintinline{python}{crossover} function.}\label{fig:crossover_implementation}
\end{figure}

\begin{figure}[!htbp]
    \begin{usagepy}
>>> import random
>>> import sequence_sensei as ss

>>> turns = 10
>>> s_one = [1 for _ in range(turns)]
>>> s_two = [i % 2 for i in range(turns)]

>>> random.seed(0)
>>> new_member = ss.crossover(s_one, s_two)
>>> new_member
[1, 1, 1, 1, 1, 1, 0, 1, 0, 1]

\end{usagepy}
\caption{An example of using \mintinline{python}{crossover} function to crossover \(S_1\) and \(S_2\)}\label{fig:crossover_usage}
\end{figure}

Following the crossover between two members, a mutation is applied to new member
before it is are added to the population. Mutation has been implemented as a
given probability \(p_m\) that each gene of the new member is flipped. A total
of \(N\) random numbers between \([0, 1]\) are sampled. If the sampled
probability at time \(i\) in less than \(p_m\) then the \(i^{th}\) gene of the
individual is flipped, as demonstrated by
Figure~\ref{fig:mutation_diagrammatic_example}.

\begin{figure}[!htbp]
    \centering
    \includestandalone[width=.6\textwidth]{src/chapters/06/tex/mutation_example}
    \caption{Mutation example of \(S_3\).}\label{fig:mutation_diagrammatic_example}
\end{figure}

The implementation of mutation in \mintinline{python}{sequence_sensei} is given
by Figure~\ref{fig:mutation_implementation}, and an example of mutating \(S_3\)
using the function is given in Figure~\ref{fig:mutation_usage}.

\begin{figure}[!htbp]
    \begin{sourcepy}
def mutation(gene, mutation_probability):
    if random.random() < mutation_probability:
        return abs(gene - 1)
    return gene
\end{sourcepy}
\caption{Source code of the \mintinline{python}{mutation} function.}\label{fig:mutation_implementation}
\end{figure}

\begin{figure}[!htbp]
    \begin{usagepy}
>>> new_member = [1, 1, 1, 1, 1, 1, 0, 1, 0, 1]

>>> random.seed(1)
>>> [ss.mutation(gene, mutation_probability=0.05) for gene in new_member]
[1, 1, 1, 1, 1, 1, 0, 1, 0, 0]

\end{usagepy}
\caption{An example of using the \mintinline{python}{mutation} function to mutate
\(S_3\).}\label{fig:mutation_usage}
\end{figure}

The main function implemented in \mintinline{python}{sequence_sensei} for
performing a GA is the \mintinline{python}{evolved} function. The function has
several input arguments which correspond to the inputs of
Algorithm~\ref{algorithm:genetic_algorithm}. In the following section the
\mintinline{python}{evolved} function is used to run several trials and estimate
best response sequence. The parameters' values for each run will be presented there.
Moreover, the details of the best response sequence collection against the
\numberofstrategiesbestsequences strategies are presented in the following section.

\section{Data Collection}\label{section:generating_sequences}

The data set generated in this Chapter was created using the GA and the
\mintinline{python}{sequence_sensei} packaged introduced in
section~\ref{section:genetic_algorithm}, and the APL project. The GA of Algorithm
\ref{algorithm:genetic_algorithm} estimates the best response sequence for a given
opponent, and in order to generate a collection of best responses a list of
opponents is obtained from APL. More specifically \numberofstrategiesbestsequences strategies are used
in this Chapter. These can be found in the Appendix.

The APL project is also used to calculate the average score per turn (\(U(S, Q)\))
player \(S\) can achieve against an opponent \(Q\). The project contains a specific
class that can simulate the play any given sequence of Cs and Ds. The player class
is called Cycler and the class takes as an argument a series of actions in a
string format. An example of creating and using such player is given by
Figure~\ref{fig:apl_simulations_cycler}. Once a match has been simulated
\(U\) can be obtained using an in build method of APL.

\begin{figure}[!htbp]
    \begin{usagepy}
>>> import axelrod as axl

>>> players = [axl.Cycler('DDDCCCDDCC'), axl.Cooperator()]
>>> match = axl.Match(players, turns=10)
>>> match.play()
[(D, C), (D, C), (D, C), (C, C), (C, C), (C, C), (D, C), (D, C), (C, C), (C, C)]

>>> match.final_score_per_turn()
(4.0, 1.5)

>>> players = [axl.Cycler('DDDCCCDDCC'), axl.TitForTat()]
>>> match = axl.Match(players, turns=10)
>>> match.play()
[(D, C), (D, D), (D, D), (C, D), (C, C), (C, C), (D, C), (D, D), (C, D), (C, C)]

>>> match.final_score_per_turn()
(2.2, 2.2)

\end{usagepy}
\caption{Simulating a match between Cycler and Cooperator and Cycler and Tit For Tat.
The class Cycler takes a given sequence as an input argument in a string format
('DDDCCCDDCC'). Once a match has been simulate with the \mintinline{python}{play} method
the average score per turn is obtained using the \mintinline{python}{final_score_per_turn}
method.}\label{fig:apl_simulations_cycler}
\end{figure}

From the \numberofstrategiesbestsequences strategies, 62 are stochastic and
135 are deterministic. In section~\ref{section:ipd_as_sequences} it was explained
that the outcome of match between two deterministic strategies is always the same.
In comparison, the outcome of match with even a single stochastic strategy can
differ.
The concept of a stochastic opponent can be somewhat ``overridden'' by using
\textit{computer seeding} for seeding the pseudo random number generator (PRNG) that creates the
parameters that define what moves the strategy will take. Seeds are set before
generating a random number, and if the same seed is used on initialisation then
the random output remains the same. Thus, as long as a match is being seeded the
behaviour of a stochastic strategy can be reproduced, and different seeds lead
to different series of random numbers.

Consider the between Random and \(S = \{D, D, D, C, C, C, D, D, C, C\}\)
presented in section~\ref{section:ipd_as_sequences}. Random has a total of
\(2^{10}\) possible plays. Using different computer seeds a number of these
plays can be simulated and reproduced, Figure~\ref{fig:random_apl_example}.

\begin{figure}[!htbp]
    \begin{usagepy}
>>> players = [axl.Cycler('DDDCCCDDCC'), axl.Random()]
>>> for seed in range(5):
...   axl.seed(seed)
...   match = axl.Match(players, turns=10)
...   actions = match.play()
...   print(actions, match.final_score_per_turn())
...   print("================================================================================")
[(D, D), (D, D), (D, C), (C, C), (C, D), (C, C), (D, D), (D, C), (C, C), (C, D)] (2.2, 2.2)
================================================================================
[(D, C), (D, D), (D, D), (C, C), (C, C), (C, C), (D, D), (D, D), (C, C), (C, C)] (2.4, 1.9)
================================================================================
[(D, D), (D, D), (D, C), (C, C), (C, D), (C, D), (D, D), (D, C), (C, D), (C, D)] (1.6, 2.6)
================================================================================
[(D, C), (D, D), (D, C), (C, D), (C, D), (C, C), (D, C), (D, D), (C, C), (C, C)] (2.6, 2.1)
================================================================================
[(D, C), (D, C), (D, C), (C, C), (C, C), (C, C), (D, D), (D, D), (C, D), (C, C)] (2.9, 1.9)
================================================================================

\end{usagepy}
\caption{Example code of using seeding to generate different plays of Random.
The value of seed changes to \{0, 1, 2, 3, 4, 5\} and the seed is set with
the command axl.seed(seed) before simulating the game. This initialises the
pseudo random generator that define what moves Random will take.
The above code snipped will always have the same output each time it is
repeated.}\label{fig:random_apl_example}
\end{figure}

A total of 10 different plays are captured for each of the stochastic strategies
of this Chapter. Thus, a total of 10 different seeds are used for each strategy.

The data collection process is given by Figure~\ref{fig:data_generating_process_diagram}.

\begin{figure}[!htbp]
    \centering
    \includestandalone[width=\textwidth]{src/chapters/06/tex/data_generating_diagram}
    \caption{Diagrammatical representation of the best response
    sequences generating process.}\label{fig:data_generating_process_diagram}
\end{figure}

From the collections of opponents a strategy is selected at each trial. If the
strategy is deterministic a set of GAs with different parameters values are
performed for 2000 generations. The summary of each generation, for each GA
trial, is written to a csv file and the next opponent is selected. If the
opponent is stochastic the above process is repeated 10 times. Each time the
stochastic strategy is accompanied by a different seed value which is used to
initialise the pseudo random generator before the strategy plays against the
members of the population. For each stochastic strategy and each seed value the
generations' details for each GA trial are written to individual csv files.

For each opponent, or opponent-seed combination a total of 18 GAs are performed.
The different values for each parameter are given by
Table~\ref{table:parameters_summary}.

\begin{table}[!htbp]
    \begin{center}
    \resizebox{.5\textwidth}{!}{
    \begin{tabular}{cccccc} \toprule
Parameter         & Explanation                   &  Values  \\ \midrule
\(N\)             & number of turns               &  205 \\
\(G\)             & number of generations         &  2000 \\
\(b\)             & bottleneck                    &  10, 20 \\
\(K\)             & size of a population          &  20, 30, 40 \\
\(p_m\)           & probability of gene mutating  &  0.01, 0.05, 0.1 \\ \bottomrule
    \end{tabular}}
    \end{center}
    \caption{The parameters of the genetic algorithm.}\label{table:parameters_summary}
\end{table}

All the best response sequences generated are best response sequences to the
strategies for \(205\) turns. Moreover, the best response sequences are not to
\numberofstrategiesbestsequences opponents but more specifically because of the
seeding (\(135 + 62 \times 10\)) 755. Thus, a total of 755 trials of
Figure~\ref{fig:data_generating_process_diagram} have been performed.

An example of an exported summary of the GA trials for a deterministic strategy
is given by Table~\ref{table:alternator_summary}.

\begin{table}[!htbp]
    \resizebox{\textwidth}{!}{
    \input{src/chapters/06/tex/alternator_table.tex}}
    \caption{An exampled of an exported summary. The specific output is for
    the opponent Alternator. Alternator is a deterministic strategy,
    consequently, the value of seed in NaN. The values of the different GA
    parameters are recorded in the summary, as well as the details of each
    member of each generation. The best responses sequences are the individuals
    that have the maximum score at \(G_i\) = 2000.}\label{table:alternator_summary}
\end{table}

For a stochastic strategy there are a total of 9 exported summaries. An example
of the export summary when the selected opponent was Champion is given by
Table~\ref{table:champion_summary}.

\begin{table}[!htbp]
    \resizebox{\textwidth}{!}{
    \input{src/chapters/06/tex/champion_seed_nine_table.tex}}
    \caption{An exampled of an exported summary for a stochastic strategy. The
    column seed does not have a value of NaN anymore but records the seed that
    was used to generate the specific play of the stochastic opponent.}\label{table:champion_summary}
\end{table}


\subsection{Data Collection Error}

The data collection process of this Chapter was carried out using parallel
programming. In parallel programming many calculations or executions of
tasks are carried out simultaneously. In the case of the data collection here
the tasks that were parallelised was the scoring process the population. More
specifically, at most 10 members of the population were being scored at the same
time.

Parallel programming was executed using \textit{multi
threading}~\cite{Shameem2005}. Threads are“light-weight” processes, a unit of
execution within a process.Threads have shared memory and they can manipulate
global variables of main thread. A task of scoring a member against a given
strategy was executed in an individual thread. In the case of the stochastic
opponents, the tasks were designed so that the PRNG was set for each thread
before simulating a match. Unfortunately, the seeding process was not done in a
safe thread way. Each thread was designed to set the global PRNG, which is then
shared across all the threads without synchronisation. Since the threads are
running in parallel, at the same time, and their access to this global PRNG is
not synchronised between them errors occur.

There are several ways that this error can be corrected. The solutions include
using multiprocessing instead of threading, that runs another subprocess in
memory and it does not have shared memory like threading. Or Giving each thread
its own PRNG solves this problem because there is no longer any state that's
shared by multiple threads without synchronisation.

\subsection{The collection of best response sequences}

Never th less, the collections process
(Figure~\ref{fig:data_generating_process_diagram}) was performed for trials.
Each trial exported a summary of the history of the GA generations. The
sequences at the final generation with the highest score are the estimated best
responses.

All sequences considered here have been of 205 turns, and the given choice will
be explained the following chapter. For each of the deterministic opponent the
best response sequences were estimated correctly. As discussed in
section~\ref{section:ipd_as_sequences}the
best response to the strategy is Tit For Tat is that of:

\[\{\underbrace{C, \dots, C}_{N-1}, D\}\]

That was correctly identified by the collection process.
Figure~\ref{fig:tit_for_tat_ga_score} shows the maximum score achieved by a
member of the population at each generation. In the case of Tit For Tat the GA
convergence was at generation 23.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.65\textwidth]{src/chapters/06/img/maximum_score_per_generation_tit_for_tat.pdf}
    \caption{The maximum score a sequence achieved against Tit For Tat over the generations.
    Each line represents a different GA trial. The trails that managed to identify the
    the best response against Tit For Tat are included.}\label{fig:tit_for_tat_ga_score}
\end{figure}

In section~\ref{section:ipd_as_sequences} is was explained that several
strategies have more than a single best response sequence. In the case of the
strategy Adaptive, any strategy that cooperated twice in the opening 6 moves and
defected thereafter is a best response. The data collection has managed to
identify multiple best responses. For example in the case of Adaptive the
following sequences were the sequences with the highest average score per turn
in the final generation.

\begin{table}[!htbp]
    \resizebox{\textwidth}{!}{
    \input{src/chapters/06/tex/best_responses_adaptive_table.tex}}
    \caption{Best responses sequences estimated by the data collection process.}
    \label{table:adaptive_best_responses}
\end{table}

Several GA trial managed to converged to best response sequence for Adaptive. A
smaller population in the case of Adaptive converged faster $(g_i)$ than a
larger one, Figure~\ref{fig:adaptive_ga_score}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.7\textwidth]{src/chapters/06/img/maximum_score_per_generation_adaptive.pdf}
    \caption{The maximum score a sequence achieved against Tit For Tat over the generations.
    Each line represents a different GA trial. The trails that managed to identify the
    the best response against Tit For Tat are included.}\label{fig:adaptive_ga_score}
\end{figure}

There are a total of 130 deterministic opponents. Several of those have numerous
best response sequences. More specifically, a total of 2949 best response
sequences have been collected for the deterministic opponents of the collection.

Due to the error which occurred for the stochastic opponents during the data
collection it is not guaranteed that the best response sequences have been estimated
for the seeded version of the stochastic players. Several sequences have still
been obtained from the collections process. More specifically, for the 62 \(\times\)
10 stochastic opponents a total of 2309 have been estimated. The final sequence
with the maximum score is accurate 66\% of the time.

For instance the strategy Champion while being seeded with \(seed=9\). The best
responses is shown in Figure~\ref{fig:champion_ga_score}. The spiked instances
are instance where the seeding process occurred. The final sequence managed
to score a to the opponent.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.7\textwidth]{src/chapters/06/img/maximum_score_per_generation_champion.pdf}
    \caption{The maximum score a sequence achieved against Tit For Tat over the generations.
    Each line represents a different GA trial. The trails that managed to identify the
    the best response against Tit For Tat are included.}\label{fig:champion_ga_score}
\end{figure}

Instance where the final sequence does and it's score do not represent the correct
seeded version are of Random with \(seed=1\).

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.7\textwidth]{src/chapters/06/img/maximum_score_per_generation_random.pdf}
    \caption{The maximum score a sequence achieved against Tit For Tat over the generations.
    Each line represents a different GA trial. The trails that managed to identify the
    the best response against Tit For Tat are included.}\label{fig:random_ga_score}
\end{figure}

From the list of 192 strategies, 750 different opponents, a total of 5258 best
response sequences of length 205 were retrieved.


\section{Chapter Summary}

This Chapter has explored the concept of best responses in the IPD game in the
form of static sequences of moves. It has identified a successful method of
generating best response sequences using an evolutionary algorithm and executed
analysis to find these solutions to the majority of opponents listed in the APL.

More specifically, a total of \numberofstrategiesbestsequences opponents from
APL were used. Several of the strategies in the project are stochastic and
computer seeded version of these strategies were used to explore their different
behaviours. A given opponent can have more than a single best response to it, and
as a result, a total of 5503 best response sequences to a total of 755 opponents
have been calculated. The data set has been archive and is available at
\cite{Glynatsi2020_sequences}.

The main purpose of this Chapter has been to generate the bespoke data set,
which is to be used as training data set in the following Chapter.
